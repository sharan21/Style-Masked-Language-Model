{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess a dataset\n",
    "def create_qqp(split=0.9):\n",
    "    x = pd.read_csv(\"./data/annotated/qqp/questions.csv\")\n",
    "    q1, q2 = list(x.question1), list(x.question2)\n",
    "    labels = list(x.is_duplicate)\n",
    "    train, test, valid = [], [], []\n",
    "    trainl, testl, validl = (int)(0.9 * len(q1)), (int)(0.05 * len(q1)), (int)(0.05 * len(q1))\n",
    "\n",
    "    for i in range(trainl):\n",
    "        try:\n",
    "            train.append(clean_line(q1[i]) + ' . ' + clean_line(q2[i]) + ' , ' + str(labels[i]) + '\\n')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    for i in range(trainl, trainl+testl, 1):\n",
    "        try:\n",
    "            test.append(clean_line(q1[i]) + ' . ' + clean_line(q2[i]) + ' , ' + str(labels[i]) + '\\n')\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for i in range(trainl+testl, len(q1), 1):\n",
    "        try:\n",
    "            valid.append(clean_line(q1[i]) + ' . ' + clean_line(q2[i]) + ' , ' + str(labels[i]) + '\\n')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    def write_to_disk(lines, output):\n",
    "        txt = open(output, 'w')\n",
    "        for line in lines:\n",
    "            txt.write(line.lower())\n",
    "    \n",
    "    write_to_disk(train, './data/annotated/qqp/train.csv')\n",
    "    write_to_disk(test, './data/annotated/qqp/test.csv')\n",
    "    write_to_disk(valid, './data/annotated/qqp/valid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix references in style-masked/yelp to make sure it aligns with test.txt\n",
    "from utils import clean_line\n",
    "lbls = ['0','1']\n",
    "datasets = ['amazon-cleaned']\n",
    "for d in datasets:\n",
    "    for lbl in lbls:\n",
    "        dst = '1' if lbl == '0' else '0'\n",
    "        path1 = './data/style-masked/{}/{}/reference.{}to{}'.format(d, lbl, lbl, dst)\n",
    "        m = {}\n",
    "        with open(path1, 'r') as f:\n",
    "            for line in f:\n",
    "                base, human = line.split('\\t')\n",
    "                base = clean_line(base)\n",
    "                base = ' '.join(base.split())\n",
    "                m[base] = human\n",
    "        path2 = './data/style-masked/{}/{}/reference.{}to{}.cleaned'.format(d, lbl, lbl, dst)\n",
    "        path3 = './data/style-masked/{}/{}/test.txt'.format(d, lbl)\n",
    "        \n",
    "        with open(path2, 'w') as f1:\n",
    "            with open(path3, 'r') as f2:\n",
    "                for line in f2:\n",
    "                    line = clean_line(line)\n",
    "                    line = ' '.join(line.split())\n",
    "                    f1.write(m[line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run nlg evaluation for SMLM models. produces nlg_evals_humans\n",
    "from nlgeval import compute_metrics\n",
    "def nlg_eval_human(model_name, dataset):\n",
    "    baseline = os.path.join('checkpoints', 'style-masked', dataset, model_name, 'exps_model_best.pt')\n",
    "    avg_metrics = {}\n",
    "    lbls = ['0', '1']\n",
    "    for src_lbl in lbls:\n",
    "        dst_lbl = '1' if src_lbl == '0' else '0'\t\n",
    "        path1 = os.path.join(baseline, 'test.{}to{}'.format(src_lbl, dst_lbl))\n",
    "        path2 = os.path.join(\"./data/style-masked/{}/{}\".format(dataset, src_lbl), 'reference.{}to{}.cleaned'.format(src_lbl, dst_lbl))\n",
    "        print(\"performing nlg eval between {} and {}\".format(path1, path2))\n",
    "        metrics = compute_metrics(hypothesis=path1, references=[path2], no_skipthoughts=True, not_naturalness=True)\n",
    "        print(metrics)\n",
    "        with open(path1+'.nlg-evals', 'w') as f:\n",
    "            for k in metrics:\n",
    "                if(k not in avg_metrics):\n",
    "                    avg_metrics[k] = metrics[k]\n",
    "                else:\n",
    "                    avg_metrics[k] += metrics[k]\n",
    "                f.write('{}: {} \\n'.format(k, metrics[k]))\n",
    "    with open(os.path.join(baseline, 'nlg_evals_human'), 'w') as f:\n",
    "        for k in metrics:\n",
    "            f.write('{}: {} \\n'.format(k, avg_metrics[k]/2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['amazon-cleaned']\n",
    "for dataset in datasets:\n",
    "    nlg_eval_human('smlmg-c10-eps0.15', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = ['0', '1']\n",
    "\n",
    "for lbl in lbls:\n",
    "    dst_lbl = '0' if lbl == '1' else '1'\n",
    "    cleaned_test = './data/annotated/amazon-cleaned/sentiment.test.{}'.format(lbl)\n",
    "    full_refs = './data/annotated/amazon-cleaned/references.{}to{}'.format(lbl, dst_lbl)\n",
    "\n",
    "    m = {}\n",
    "\n",
    "    with open(full_refs, 'r') as f:\n",
    "        for line in f:\n",
    "            base, human = line.split('\\t')\n",
    "            base = clean_line(base)\n",
    "            base = ' '.join(base.split())\n",
    "            m[base] = human\n",
    "\n",
    "    for i, line in enumerate(m):\n",
    "        if(i==5):\n",
    "            break\n",
    "        print(line)\n",
    "    \n",
    "    cleaned_refs = './data/annotated/amazon-cleaned/references.{}to{}.cleaned'.format(lbl, dst_lbl)\n",
    "\n",
    "    with open(cleaned_test, 'r') as f1:\n",
    "        with open(cleaned_refs, 'w') as f2:\n",
    "            for line in f1:\n",
    "                old_line = line\n",
    "                line = clean_line(line)\n",
    "                line = ' '.join(line.split())\n",
    "                f2.write(old_line + '\\t' + m[line])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get acciracy% metrics for attri method comparison table\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.translate.bleu_score import sentence_bleu \n",
    "from vocab import Vocab\n",
    "from model import *\n",
    "from utils import *\n",
    "from preproc import create_toy_dataset\n",
    "from batchify import *\n",
    "from nlgeval import compute_metrics\n",
    "import nltk\n",
    "\n",
    "def load_sent(path): # used to load sents from unannotated datasets with format: {input}\n",
    "\tinputs = []\n",
    "\twith open(path) as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.split(',')[0]\n",
    "\t\t\tinputs.append(line.split())\n",
    "\treturn inputs\n",
    "\n",
    "def classify(sents, model, args, batch_size=32):\n",
    "\t# assert(args.classifier_checkpoint != None)\n",
    "\tprint(\"Predicting labels for {} sentences...\".format(len(sents)))\n",
    "\tbatches, _ = get_batches(sents, classifier_vocab, batch_size, torch.device('cpu'), sort=False)\n",
    "\tpreds = []\n",
    "\tfor i, (inputs, _) in enumerate(batches):\n",
    "\t\t_, _, outs = model(inputs, args)  # BO\n",
    "\t\touts = torch.argmax(outs, dim=-1)  # B1\n",
    "\t\tpreds.append(outs.detach().cpu().numpy())\n",
    "\tpreds = np.concatenate(preds, axis=0)\n",
    "\treturn preds\n",
    "\n",
    "def get_model(path, vocab):\n",
    "\tprint(\"loading model from path {}\".format(path))\n",
    "\tckpt = torch.load(path)\n",
    "\ttrain_args = ckpt['args']    \n",
    "\tmodel = {'dae': DAE, 'vae': VAE, 'aae': AAE, 'van': VanillaAE, \n",
    "\t\t'lstmclassifier': LSTMClassifier, 'divlstmclassifier':DiversityLSTMClassifier, \n",
    "\t\t'tr-encoder':TransformerModel}[ \n",
    "\t\ttrain_args.model_type](vocab, train_args).to(torch.device('cpu'))\n",
    "\tmodel.load_state_dict(ckpt['model'])\n",
    "\tif(not isinstance(model, TransformerModel)):\n",
    "\t\tmodel.flatten()\n",
    "\treturn model, train_args\n",
    "\n",
    "def trythis(data, classifier_model, classifier_args, lbl):\n",
    "    sents = load_sent(data)\n",
    "    preds = classify(sents, classifier_model, classifier_args)\n",
    "    wrong = 0\n",
    "    acc = 0\n",
    "    with open(data+'.preds', 'w') as f:\n",
    "        for i, p in enumerate(preds):\n",
    "            if(p != lbl):\n",
    "                wrong += 1\n",
    "            f.write(' '.join(sents[i]) + '\\t' + str(p) + '\\n')\n",
    "        acc = (len(preds) - wrong)/len(preds)\n",
    "        f.write('Style Transfer Accuracy: {} \\n'.format(str(acc)))\n",
    "    return acc\n",
    "\n",
    "dataset = 'snli'\n",
    "r = [0.0,0.1,0.2,0.3,0.4,0.5]\n",
    "for i in r:\n",
    "\tckpt = 'snli-{}'.format(i)\n",
    "\tclassifier_checkpoint = './checkpoints/supervised/{}/classifier'.format(dataset)\n",
    "\tif(classifier_checkpoint != None): #used in TST exp mainly\n",
    "\t\tclassifier_vocab = Vocab(os.path.join(classifier_checkpoint, 'vocab.txt'))\n",
    "\t\tclassifier_model, classifier_train_args = get_model(os.path.join(classifier_checkpoint, 'checkpoints', 'model_best.pt'), classifier_vocab)\n",
    "\tlbls = ['0','1']\n",
    "\tm = 0\n",
    "\tfor lbl in lbls:\n",
    "\t\tdata = './data/style-masked/{}/{}/test.csv'.format(ckpt, lbl)\n",
    "\t\tacc = trythis(data, classifier_model, classifier_train_args, int(lbl))\n",
    "\t\tm += acc\n",
    "\tprint(m/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot line graphs for eps analysis exp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "figure, a = plt.subplots(2, figsize=(10,10))\n",
    "x = [0.0,0.1,0.2,0.3,0.4,0.5]\n",
    "yelp = [55,62,66,70,72,75]\n",
    "imdb = [float(e) for e in \"54\t65\t73\t78\t81\t84\".split()]\n",
    "amazon = [float(e) for e in \"48\t67\t78\t84\t88\t91\".split()]\n",
    "snli = [float(e) for e in \"56\t65\t72\t78\t82\t85\".split()]\n",
    "a[0].grid()\n",
    "a[0].plot(x, yelp, marker='.')\n",
    "a[0].plot(x, imdb, marker='o')\n",
    "a[0].plot(x, amazon, marker='^')\n",
    "a[0].plot(x, snli, marker='v')\n",
    "a[0].set_xlabel(\"$\\lambda_{eps}$\")\n",
    "a[0].set_ylabel(\"s-BLEU\")\n",
    "a[0].legend([\"Yelp\", \"IMDb\", \"Amazon\", \"SNLI\"], loc =\"lower right\")\n",
    "\n",
    "yelp = [float(e)*100 for e in \"0.69\t0.73\t0.73\t0.75\t0.77\t0.77\".split()]\n",
    "imdb = [float(e)*100 for e in \"0.722\t0.75\t0.78\t0.79\t0.81\t0.82\".split()]\n",
    "amazon = [float(e)*100 for e in \"0.71\t0.76\t0.79\t0.82\t0.84\t0.85\".split()]\n",
    "snli = [float(e)*100 for e in \"0.49\t0.49\t0.52\t0.56\t0.63\t0.67\".split()]\n",
    "a[1].plot(x, yelp, marker='.')\n",
    "a[1].plot(x, imdb, marker='o')\n",
    "a[1].plot(x, amazon, marker='^')\n",
    "a[1].plot(x, snli, marker='v')\n",
    "a[1].set_xlabel(\"$\\lambda_{eps}$\")\n",
    "a[1].set_ylabel(\"Accuracy(\\%)\")\n",
    "a[1].grid()\n",
    "\n",
    "figure.savefig('./media/eps-effect.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data to suite tag and generate tsv form\n",
    "import os\n",
    "\n",
    "dataset = 'snli'\n",
    "labels = ['0', '1']\n",
    "splits = ['train', 'test', 'valid']\n",
    "base = os.path.join('./data/annotated', dataset)\n",
    "out = os.path.join(base, '{}-tag-gen.tsv'.format(dataset))\n",
    "sents = []\n",
    "\n",
    "stylemap = {'0':'neg', '1':'pos'}\n",
    "\n",
    "\n",
    "with open(out, 'w') as f:\n",
    "    f.write(\"txt\\tsplit\\tstyle\\n\") #format\n",
    "    for split in splits:\n",
    "        for lbl in labels:\n",
    "            inpath = os.path.join(base, 'nli.{}.{}'.format(split, lbl))\n",
    "            print(\"importing from {}\".format(inpath))\n",
    "            \n",
    "            with open(inpath, 'r') as inp:\n",
    "                for line in inp:\n",
    "                    # print(line)\n",
    "                    s = \"val\" if split == \"valid\" else split\n",
    "                    l = line[:-1] + '\\t' + s + '\\t' + stylemap[lbl]\n",
    "                    sents.append(l)\n",
    "                    f.write(l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For EtoC direction\n",
      "./human-evals/pooja/EtoC/EtoC.0\n",
      "./human-evals/sharan/EtoC/EtoC.0\n",
      "./human-evals/pooja/EtoC/EtoC.1\n",
      "./human-evals/sharan/EtoC/EtoC.1\n",
      "./human-evals/pooja/EtoC/EtoC.2\n",
      "./human-evals/sharan/EtoC/EtoC.2\n",
      "./human-evals/pooja/EtoC/EtoC.3\n",
      "./human-evals/sharan/EtoC/EtoC.3\n",
      "./human-evals/pooja/EtoC/EtoC.4\n",
      "./human-evals/sharan/EtoC/EtoC.4\n",
      "{'1': 8, '2': 38, '3': 8, '4': 14, 'na': 32}\n",
      "For CtoE direction\n",
      "./human-evals/pooja/CtoE/CtoE.0\n",
      "./human-evals/sharan/CtoE/CtoE.0\n",
      "./human-evals/pooja/CtoE/CtoE.1\n",
      "./human-evals/sharan/CtoE/CtoE.1\n",
      "./human-evals/pooja/CtoE/CtoE.2\n",
      "./human-evals/sharan/CtoE/CtoE.2\n",
      "./human-evals/pooja/CtoE/CtoE.3\n",
      "./human-evals/sharan/CtoE/CtoE.3\n",
      "./human-evals/pooja/CtoE/CtoE.4\n",
      "./human-evals/sharan/CtoE/CtoE.4\n",
      "{'1': 3, '2': 31, '3': 1, '4': 13, 'na': 52}\n"
     ]
    }
   ],
   "source": [
    "#evaluate forms\n",
    "from collections import Counter\n",
    "\n",
    "dataset = 'snli'\n",
    "scores = {'1':0, '2':0, '3':0, '4':0, 'na':0}\n",
    "\n",
    "def get_maj(l):\n",
    "    c = Counter(l)\n",
    "    value1, count1 = c.most_common()[0]\n",
    "    if(len(c.most_common()) == 1):\n",
    "        return value1\n",
    "    value2, count2 = c.most_common()[1]\n",
    "    maj = -1 if count1 == count2 else value1\n",
    "    return maj\n",
    "\n",
    "def get_vals(path):\n",
    "    vals = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            if(len(words) != 0 and words[0] == \"Decision:\"):\n",
    "                vals.append(int(words[1]))\n",
    "    assert(len(vals) == 20)\n",
    "    return vals\n",
    "\n",
    "def print_results(reset=False):\n",
    "    global scores\n",
    "    print(scores)\n",
    "    if(reset):\n",
    "        scores = {'1':0, '2':0, '3':0, '4':0, 'na':0}\n",
    "    return\n",
    "\n",
    "people = ['pooja','sharan']\n",
    "t = {p: None for p in people}\n",
    "lbls = ['E', 'C']\n",
    "ids = [e for e in range(5)]\n",
    "\n",
    "def update(): #take 20 votes from each person and update votes for one file\n",
    "    global t, scores\n",
    "    for i in range(20):\n",
    "        l = []\n",
    "        for p in people:\n",
    "            l.append(t[p][i])\n",
    "        maj = get_maj(l)\n",
    "        if(maj == -1):\n",
    "            scores['na'] += 1\n",
    "        else:\n",
    "            scores[str(maj)] += 1\n",
    "    \n",
    "\n",
    "for src_lbl in lbls:\n",
    "    for dst_lbl in lbls:\n",
    "        if(src_lbl == dst_lbl):\n",
    "            continue\n",
    "        print('For {}to{} direction'.format(src_lbl, dst_lbl))\n",
    "        base_dir = './human-evals/{}/{}to{}/'.format(dataset, src_lbl, dst_lbl)\n",
    "        for id in ids:\n",
    "            form = '{}to{}.{}'.format(src_lbl, dst_lbl, id)\n",
    "            for p in people:\n",
    "                form_path = './human-evals/{}/{}to{}/{}to{}.{}'.format(p, src_lbl, dst_lbl, src_lbl, dst_lbl, id)\n",
    "                print(form_path)\n",
    "                t[p] = get_vals(form_path)\n",
    "            \n",
    "            #after you get all the votes for everyone for a form, get results of that form\n",
    "            update()\n",
    "        #after {}to{} dir is over, print stats\n",
    "        print_results(reset=True)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l = [2,1,1,2]\n",
    "l = [2,1,1,2]\n",
    "c = Counter(l)\n",
    "value1, count1 = c.most_common()[0]\n",
    "value2, count2 = c.most_common()[1]\n",
    "\n",
    "maj = -1 if count1 == count2 else value1\n",
    "print(maj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
